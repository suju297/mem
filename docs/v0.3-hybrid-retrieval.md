# v0.3 Hybrid Retrieval (MCP-first)

## Goals
- Keep MCP API unchanged (`get_context`, `explain`).
- Make vectors opt-in and degradable (BM25-only when no embedder).
- Use RRF fusion with existing recency/thread bonuses.
- Add a similarity threshold to drop low-confidence vector-only matches.
- Keep explain output first-class for debugging.

## Non-goals
- No breaking CLI changes.
- Only Ollama provider is implemented initially; other providers remain stubs.

## Config
```toml
embedding_provider = "auto" # auto|none|ollama|python|onnx
embedding_model = "nomic-embed-text"
embedding_min_similarity = 0.6
```

Notes:
- `embedding_provider = auto` uses Ollama when reachable; otherwise it falls back to BM25-only.
- Mempack never auto-downloads models; `ollama pull <model>` is user-initiated.
- `embedding_provider = none` forces BM25-only.
- `embedding_min_similarity` applies to vector-only additions, not BM25-backed hits.
- RRF parameters are internal defaults (k=60, weight=60) for now.
- Run `mem embed --kind all` after enabling a provider to backfill embeddings.

## Data Flow (get_context/explain)
1. Run FTS (BM25) search for memories + chunks.
2. If an embedder is configured and available:
   - Embed query text.
   - Vector search embeddings for memories/chunks.
   - Drop vector-only results below `embedding_min_similarity`.
3. Merge FTS and vector results (fetch detail rows for vector-only ids).
4. Rank with RRF fusion + recency/thread bonuses (and safety penalties).
5. Budget pack and emit context as usual.
6. Explain output includes FTS rank, vector rank/score, and RRF score.

## Migration Plan
- v0.3 adds an embeddings table keyed by `{repo_id, workspace, kind, item_id, model}`.
- Backfill with `mem embed --kind all` when a provider is enabled.
- Rebuild embeddings when the embedding model changes.
